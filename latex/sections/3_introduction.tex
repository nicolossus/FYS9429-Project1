%================================================================
\section{Introduction}\label{sec:Introduction}
%================================================================

When working with high-dimensional data, analysis and modeling often necessitate the compression of the data into a lower-dimensional representation. It is then crucial to extract the relevant information. The features of high-dimensional data can be expertly crafted using domain knowledge or found by tapping into the ability of modern machine learning methods to learn useful representations of the data directly.

In this project, the objective is to investigate whether variational autoencoders (VAEs) \citep{kingma2022autoencoding} can be used to learn features from simulated neural recordings with the Hodgkin-Huxley model \citep{HH1952}. We embark on this quest in the traditional way -- by building simple models and testing them on the (binarized) MNIST dataset. Specifically, we will build VAEs with simple dense and convolutional neural networks as encoder/decoder architecture. In addition to reducing the dimensionality, it is desirable for the VAE to be able to extract meaningful and independent latent factors that explain the majority of the variation in the data. This is called disentangling the latent space and is facilitated by, for example, $\beta$-VAEs \citep{higgins2017betavae}. We will also build $\beta$-VAEs to explore the disentanglement of the latent space. Informed by experiments on the binarized MNIST dataset, we apply the VAEs on the the simulated neural data.

The report is organized as follows. In \autoref{sec:Theory} we first provide the theoretical background of variational autoencoders and the dense and convolutional neural networks we use as encoder/decoder architectures. We also provide some brief background necessary to understand the neural data on which we ultimately will train the VAE model. In \autoref{sec:Method} we present the datasets and the different VAE architectures used, as well as a short description of the methods applied in the training of the VAEs. In \autoref{sec:Results} we present and discuss the results. Finally, in \autoref{sec:Conclusion} we provide conclusions to the approach and outline possible avenues for future research.
