%----------------------------------------------------------------
\subsection*{Notes}
%---------------------------------------------------------------- 



--

Simulators are the modern manifestation of scientific theories. They can implement complex mechanistic models of natural phenomena of interest as well as models for the instruments used to observe the processes under investigation. The flexibility of simulators has made them critical research tools for testing hypotheses and predicting system dynamics in many areas of science and engineering. The central challenge in simulation-based science is constraining the parameters of the underlying mechanistic model to make the simulator's predictions consistent with empirical data. Unfortunately, simulators are poorly suited for statistical inference and lead to challenging inverse problems. Mechanistic models defined through simulators describes how a process generates data, and can thus be run forward to generate samples from the likelihood. However, while the likelihood function itself can be derived for purely statistical models, the likelihood is generally intractable or computationally infeasible for simulator models, rendering traditional methods in the toolkit of statistical inference inaccessible. To overcome the challenge of intractable likelihoods, there are an emerging set of techniques for \textit{simulation-based inference} (SBI). These methods replace explicit likelihood evaluations with model simulations and make simulator models amenable for analysis in a fully Bayesian context.

The target of a SBI algorithm can either be the posterior over model parameters directly or a surrogate model for the likelihood, which requires additional inference to obtain the posterior. The simplest example of SBI, the \textit{Approximate Bayesian Computation} (ABC) rejection method, builds an approximation to the true posterior by using repeated runs of the simulator together with an accept-reject criterion based on the discrepancy between the simulated and empirical data to draw posterior samples. Complex simulator models typically have both high-dimensional parameter and observational data spaces. Due to the curse of dimensionality, the posterior is therefore typically conditioned on a lower-dimensional representation of the empirical data. Condensing high-dimensional observations into low-dimensional summary statistics that captures information relevant to the parameters of interest is crucial to the success of the inference.

Features of high-dimensional activity of neural populations can, for example, be expertly crafted by using domain knowledge or found by tapping into the ability of modern machine learning methods to learn useful representations directly from high-dimensional data. The latter approach might be able to uncover latent structures and dynamics from the activity of neural populations.

--

Large-scale mathematical modeling and simulation of processes in nature is now ubiquitous in the physical sciences. Weather forecasts are made based on simulations of the underlying physical laws governing meteorology \cite{Bauer_2015}. Electronic and optical properties of materials are routinely computed by numerical evaluation of the Schr√∂dinger equation \cite{Giustino_2014}, while complex fluid dynamics can be computed from the Navier-Stokes equations \cite{Tu_2007}. A key feature of the computational approaches in these examples is that the underlying models and modeling schemes can be said to be \textit{multipurpose} in that they can accurately predict experiments and measurements in many different situations. The best weather models used by meteorologists are, for example, able to accurately predict the weather both by the sea and in the mountains, throughout the different seasons of the year, and even under changing climates \cite{Bauer_2015}.

In the life sciences the situation is different. Even though the physical laws underlying living matter are the same as those governing ordinary materials, mathematical modeling and simulation is much less prevalent. One exception is the use of molecular dynamics simulations to study properties of the large organic molecules on which life processes are built, such as proteins and DNA \cite{Hollingsworth_2018}. At the whole-cell level, modeling has been successfully used to model intracellular processes underlying the life cycle of a simple bacterium \cite{Karr_2012, Covert_2017}. In this pioneering and heroic effort, the main challenge was to determine the more than 1900 model parameters specifying all the chemical-reaction models governing the intracellular dynamics. This illustrates a key point: Biology is an information-rich science, and the problem of large numbers of a priori unknown model parameters is generic to the field. While only a handful of parameters (electron mass, Planck constant, etc.) may be needed to simulate electronic and optical properties of materials \cite{Giustino_2014}, the parameters describing biological systems at the cellular and systems levels are much more numerous. Further, they vary between animals and sometimes also over time in the same animal. The main challenge of making multipurpose models in biology is thus often not to set up the model equations, rather to identify suitable values for the model parameters.

Neuroscience is arguably the subfield in biology where mathematical modeling and simulation is most developed. This can partially be traced back to the seminal work of A. Hodgkin and A. Huxley who developed a breakthrough model for action potential generation and propagation in squid giant axons \cite{HH_1952}. The model, and the experimental work that led up to it, earned its authors a share of the 1963 Nobel Prize in Physiology or Medicine, establishing a new framework for thinking about the electrical activity of neurons. The model demonstrated that the generation and propagation of action potentials in neurons, the main carrier of information in the brain, can be modeled without taking into account the detailed intracellular dynamics of neurons. This dynamics could instead be included via statistical models for currents going through the various types of ion channels embedded in the membrane \cite{HH_1952}. Biophysics-based modeling of neurons with anatomically reconstructed morphologies, is now well established \cite{Koch_1999, Dayan_2001, Sterratt_2011, Gerstner_2014}. Numerous neuron models tailored to specific neuron types have been constructed, including key cells in mammalian visual cortex (see, e.g., model database at \cite{ModelDB}). These have not only provided crucial insights into the functioning of neurons, they also form a solid starting point for model-based exploration of neuronal networks.