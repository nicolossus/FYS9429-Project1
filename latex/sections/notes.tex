%----------------------------------------------------------------
\subsection*{Notes}
%---------------------------------------------------------------- 


%Tentative title: VAE-based representation learning of neural population activity

Simulators are the modern manifestation of scientific theories. They can implement complex mechanistic models of natural phenomena of interest as well as models for the instruments used to observe the processes under investigation. The flexibility of simulators has made them critical research tools for testing hypotheses and predicting system dynamics in many areas of science and engineering. The central challenge in simulation-based science is constraining the parameters of the underlying mechanistic model to make the simulator's predictions consistent with empirical data. Unfortunately, simulators are poorly suited for statistical inference and lead to challenging inverse problems. Mechanistic models defined through simulators describes how a process generates data, and can thus be run forward to generate samples from the likelihood. However, while the likelihood function itself can be derived for purely statistical models, the likelihood is generally intractable or computationally infeasible for simulator models, rendering traditional methods in the toolkit of statistical inference inaccessible. To overcome the challenge of intractable likelihoods, there are an emerging set of techniques for \textit{simulation-based inference} (SBI). These methods replace explicit likelihood evaluations with model simulations and make simulator models amenable for analysis in a fully Bayesian context.

The target of a SBI algorithm can either be the posterior over model parameters directly or a surrogate model for the likelihood, which requires additional inference to obtain the posterior. The simplest example of SBI, the \textit{Approximate Bayesian Computation} (ABC) rejection method, builds an approximation to the true posterior by using repeated runs of the simulator together with an accept-reject criterion based on the discrepancy between the simulated and empirical data to draw posterior samples. Complex simulator models typically have both high-dimensional parameter and observational data spaces. Due to the curse of dimensionality, the posterior is therefore typically conditioned on a lower-dimensional representation of the empirical data. Condensing high-dimensional observations into low-dimensional summary statistics that captures information relevant to the parameters of interest is crucial to the success of the inference.

Features of high-dimensional activity of neural populations can, for example, be expertly crafted by using domain knowledge or found by tapping into the ability of modern machine learning methods to learn useful representations directly from high-dimensional data. The latter approach might be able to uncover latent structures and dynamics from the activity of neural populations.

The idea of this project is to use Variational Autoencoders (VAEs) for dimension reduction of neural population activity to improve SBI methods for parameter identification in mechanistic models of neural dynamics. In particular, the idea is to explore:

\begin{itemize}
    \item Convolutional VAEs
    \item LSTM-based VAEs
    \item Diffusion VAEs
\end{itemize}


for feature learning from simulated extracellular recordings. Raw extracellular recordings of neural signals are composed of two main components: \textit{local field potential} (LFP) and \textit{spikes} (action potentials). Spikes are extracted from the high-frequency part of these signals and LFP from the low-frequency part. As opposed to the spike data, which represent the output of a neuron, LFPs represent the synchronized input into the observed area, accounting for the biophysics of neurons, populations, and recurrent connections.

\subsection{Mechanistic Model}

\textbf{The Hodgkin-Huxley (HH) model}

\begin{itemize}
    \item Biophysically detailed description of initiation and propagation of action potentials in squid giant axons
    \item Deterministic model
\end{itemize}

\textbf{The Brunel network model}

\begin{itemize}
    \item Sparsely connected recurrent network consisting of point neurons
    \item Excitation-inhibition balanced
    \item Stochastic model (random connections, Poisson input from external population)
\end{itemize}
    
Though both models are highly idealized, the modeling frameworks they established have been vastly influential for modern neural modeling.

\subsection{Bayesian}

In Bayesian inference, complete knowledge about a vector of model parameters, $\theta \in \Theta$, obtained by fitting a model $\mathcal{M}$, is contained in the posterior distribution. Here, prior beliefs about the model parameters, as expressed through the prior distribution, $\pi \qty(\theta)$, are updated by observing data $y_\mathrm{obs} \in \mathcal{Y}$ through the likelihood function $p  \qty(y_\mathrm{obs} | \theta)$ of the model. Using Bayes' theorem, the resulting posterior distribution

\begin{equation}
    \pi \qty(\theta | y_\mathrm{obs}) = \frac{p  \qty(y_\mathrm{obs} | \theta) \pi \qty(\theta)}{\int_\Theta p  \qty(y_\mathrm{obs} | \theta) \pi \qty(\theta) \mathrm{d} \theta}
\end{equation}

contains all necessary information required for analysis of the model checking and validation, predictive inference, and decision making. Typically, the complexity of the model and/or prior means that the posterior distribution, $\pi \qty(\theta | y_\mathrm{obs})$, is not available in closed form, and so numerical methods are needed to proceed with the inference. 

\subsection{Sum stats}

The choice of summary statistics for an ABC analysis is a critical decision that directly affects the quality of the posterior approximation. Many approaches for determining these statistics are available, and these are reviewed in Blum et al. (2013) and Prangle (2019). These methods seek to trade off aspects of the ABC posterior approximation that directly result from the choice of summary statistics. The first is that $\pi \qty(\theta | y_\mathrm{obs})$ is approximated by $\pi \qty(\theta | s_\mathrm{obs})$. As this represents an irrevocable potential information loss, the information content in $s_\mathrm{obs}$ should be high. The second aspect of the ABC posterior approximation is that the simulated and observed summary statistics are compared within a smoothing kernel $K_h \qty(\norm{s - s_\mathrm{obs}})$ as part of the form of $\pi_\mathrm{ABC} \qty(\theta | s_\mathrm{obs})$ (TODO: set up equations). As stochastically matching $s$ and $s_\mathrm{obs}$ becomes increasingly difficult as the dimension of the summary statistics increases, the dimension of $s$ should be low. As such, the the dimension of the summary statistic should be large enough so that it contains as much information about the observed data as possible, but also low enough so that the curse-of-dimensionality of matching $s$ and $s_\mathrm{obs}$ is avoided. For some models it may be the case that the dimension of the minimal sufficient statistic is equal to that of the original dataset. As this will cause curse-of-dimensionality problems in matching $s$ with $s_\mathrm{obs}$, it is likely that a more accurate ABC posterior approximation can be achieved by using a lower-dimensional non-sufficient statistic, rather than remaining within the class of sufficient summary statistics.


--

Simulators are the modern manifestation of scientific theories. They can implement complex mechanistic models of natural phenomena of interest as well as models for the instruments used to observe the processes under investigation. The flexibility of simulators has made them critical research tools for testing hypotheses and predicting system dynamics in many areas of science and engineering. The central challenge in simulation-based science is constraining the parameters of the underlying mechanistic model to make the simulator's predictions consistent with empirical data. Unfortunately, simulators are poorly suited for statistical inference and lead to challenging inverse problems. Mechanistic models defined through simulators describes how a process generates data, and can thus be run forward to generate samples from the likelihood. However, while the likelihood function itself can be derived for purely statistical models, the likelihood is generally intractable or computationally infeasible for simulator models, rendering traditional methods in the toolkit of statistical inference inaccessible. To overcome the challenge of intractable likelihoods, there are an emerging set of techniques for \textit{simulation-based inference} (SBI). These methods replace explicit likelihood evaluations with model simulations and make simulator models amenable for analysis in a fully Bayesian context.

The target of a SBI algorithm can either be the posterior over model parameters directly or a surrogate model for the likelihood, which requires additional inference to obtain the posterior. The simplest example of SBI, the \textit{Approximate Bayesian Computation} (ABC) rejection method, builds an approximation to the true posterior by using repeated runs of the simulator together with an accept-reject criterion based on the discrepancy between the simulated and empirical data to draw posterior samples. Complex simulator models typically have both high-dimensional parameter and observational data spaces. Due to the curse of dimensionality, the posterior is therefore typically conditioned on a lower-dimensional representation of the empirical data. Condensing high-dimensional observations into low-dimensional summary statistics that captures information relevant to the parameters of interest is crucial to the success of the inference.

Features of high-dimensional activity of neural populations can, for example, be expertly crafted by using domain knowledge or found by tapping into the ability of modern machine learning methods to learn useful representations directly from high-dimensional data. The latter approach might be able to uncover latent structures and dynamics from the activity of neural populations.

--

Large-scale mathematical modeling and simulation of processes in nature is now ubiquitous in the physical sciences. Weather forecasts are made based on simulations of the underlying physical laws governing meteorology \cite{Bauer_2015}. Electronic and optical properties of materials are routinely computed by numerical evaluation of the Schrödinger equation \cite{Giustino_2014}, while complex fluid dynamics can be computed from the Navier-Stokes equations \cite{Tu_2007}. A key feature of the computational approaches in these examples is that the underlying models and modeling schemes can be said to be \textit{multipurpose} in that they can accurately predict experiments and measurements in many different situations. The best weather models used by meteorologists are, for example, able to accurately predict the weather both by the sea and in the mountains, throughout the different seasons of the year, and even under changing climates \cite{Bauer_2015}.

In the life sciences the situation is different. Even though the physical laws underlying living matter are the same as those governing ordinary materials, mathematical modeling and simulation is much less prevalent. One exception is the use of molecular dynamics simulations to study properties of the large organic molecules on which life processes are built, such as proteins and DNA \cite{Hollingsworth_2018}. At the whole-cell level, modeling has been successfully used to model intracellular processes underlying the life cycle of a simple bacterium \cite{Karr_2012, Covert_2017}. In this pioneering and heroic effort, the main challenge was to determine the more than 1900 model parameters specifying all the chemical-reaction models governing the intracellular dynamics. This illustrates a key point: Biology is an information-rich science, and the problem of large numbers of a priori unknown model parameters is generic to the field. While only a handful of parameters (electron mass, Planck constant, etc.) may be needed to simulate electronic and optical properties of materials \cite{Giustino_2014}, the parameters describing biological systems at the cellular and systems levels are much more numerous. Further, they vary between animals and sometimes also over time in the same animal. The main challenge of making multipurpose models in biology is thus often not to set up the model equations, rather to identify suitable values for the model parameters.

Neuroscience is arguably the subfield in biology where mathematical modeling and simulation is most developed. This can partially be traced back to the seminal work of A. Hodgkin and A. Huxley who developed a breakthrough model for action potential generation and propagation in squid giant axons \cite{HH_1952}. The model, and the experimental work that led up to it, earned its authors a share of the 1963 Nobel Prize in Physiology or Medicine, establishing a new framework for thinking about the electrical activity of neurons. The model demonstrated that the generation and propagation of action potentials in neurons, the main carrier of information in the brain, can be modeled without taking into account the detailed intracellular dynamics of neurons. This dynamics could instead be included via statistical models for currents going through the various types of ion channels embedded in the membrane \cite{HH_1952}. Biophysics-based modeling of neurons with anatomically reconstructed morphologies, is now well established \cite{Koch_1999, Dayan_2001, Sterratt_2011, Gerstner_2014}. Numerous neuron models tailored to specific neuron types have been constructed, including key cells in mammalian visual cortex (see, e.g., model database at \cite{ModelDB}). These have not only provided crucial insights into the functioning of neurons, they also form a solid starting point for model-based exploration of neuronal networks.