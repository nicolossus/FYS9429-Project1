{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "import flax.linen as nn\n",
    "import h5py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from flax.training import train_state\n",
    "from jax import random\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from neurovae import Conv1DVAE, sse_loss, gaussian_kld, reparameterize\n",
    "from helper import fig_path\n",
    "\n",
    "sns.set_theme(context=\"paper\", style=\"darkgrid\", rc={\"axes.facecolor\": \"0.96\"})\n",
    "fontsize = \"x-large\"\n",
    "params = {\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.sans-serif\": [\"Computer Modern\"],\n",
    "    \"axes.labelsize\": fontsize,\n",
    "    \"legend.fontsize\": fontsize,\n",
    "    \"xtick.labelsize\": fontsize,\n",
    "    \"ytick.labelsize\": fontsize,\n",
    "    \"legend.handlelength\": 2,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "plt.rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size, drop_remainder):\n",
    "\n",
    "    data_size = data.shape[0]\n",
    "    remainder = data_size % batch_size\n",
    "\n",
    "    if drop_remainder and remainder != 0:\n",
    "        it = iter(data[: data_size - remainder, :])\n",
    "    else:\n",
    "        it = iter(data)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    while batch := tuple(islice(it, batch_size)):\n",
    "        batches.append(jnp.asarray(batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "latent_dim = 20\n",
    "output_dim = 10000\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "seed = 42\n",
    "\n",
    "# prepare HH data\n",
    "t_sim = 100.0\n",
    "dt = 0.01\n",
    "N = int(t_sim / dt)\n",
    "t = np.linspace(0, t_sim, N)\n",
    "\n",
    "vs = []\n",
    "for i in range(10):\n",
    "    infile = f\"./hh_data/hh_sim_data_{i}.h5\"\n",
    "    with h5py.File(infile, \"r\") as f:\n",
    "        for grp_name in f.keys():\n",
    "            v = f[grp_name][\"v\"][:]\n",
    "            v_scaled = (v - v.min(keepdims=True)) / (v.max(keepdims=True) - v.min(keepdims=True))\n",
    "            vs.append(v_scaled)\n",
    "\n",
    "v_train = vs[:8000]\n",
    "v_test = vs[8000:]\n",
    "\n",
    "# cast to jax\n",
    "v_train = jnp.asarray(v_train, dtype=jnp.float32)\n",
    "v_test = jnp.asarray(v_test, dtype=jnp.float32)\n",
    "\n",
    "batches = create_batches(v_train, batch_size, drop_remainder=True)\n",
    "\n",
    "# set values for learning rate scheduler\n",
    "total_steps = len(batches) * epochs\n",
    "init_lr = 1e-3\n",
    "alpha_lr = 1e-2\n",
    "\n",
    "\n",
    "def model():\n",
    "    return Conv1DVAE(latent_dim)\n",
    "\n",
    "\n",
    "def init_model(rng):\n",
    "    rng, init_key = random.split(rng)\n",
    "\n",
    "    initial_variables = jnp.ones((batch_size, output_dim), jnp.float32)\n",
    "    params = model().init(init_key, initial_variables, rng)[\"params\"]\n",
    "    del initial_variables, init_key\n",
    "\n",
    "    lr_schedule = optax.cosine_decay_schedule(init_lr, decay_steps=total_steps, alpha=alpha_lr)\n",
    "    optimizer = optax.chain(optax.clip(1.0), optax.adamw(lr_schedule, nesterov=True))\n",
    "\n",
    "    state = train_state.TrainState.create(\n",
    "        apply_fn=model().apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "    )\n",
    "    return rng, state\n",
    "\n",
    "\n",
    "def compute_metrics(recon_x, x, mean, logvar):\n",
    "    mse = sse_loss(recon_x, x).mean()  # mean over batch\n",
    "    kld = gaussian_kld(mean, logvar).mean()  # mean over batch\n",
    "    elbo = mse + kld\n",
    "    return {\"elbo\": elbo, \"mse\": mse, \"kld\": kld}\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, z_rng):\n",
    "\n",
    "    def loss_fn(params):\n",
    "        recon_x, mean, logvar = model().apply({\"params\": params}, batch, z_rng)\n",
    "        mse = sse_loss(recon_x, batch).mean()\n",
    "        kld = gaussian_kld(mean, logvar).mean()\n",
    "        elbo = mse + kld\n",
    "        return elbo\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_f(params, v_traces, z, z_rng):\n",
    "    def eval_model(vae):\n",
    "        recon_v_traces, mean, logvar = vae(v_traces, z_rng)\n",
    "\n",
    "        comparison = jnp.concatenate(\n",
    "            [\n",
    "                v_traces[:8],\n",
    "                recon_v_traces[:8],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        generate_v_traces = vae.generate(z, assumption=\"gaussian\")\n",
    "        metrics = compute_metrics(recon_v_traces, v_traces, mean, logvar)\n",
    "        return metrics, comparison, generate_v_traces\n",
    "\n",
    "    return nn.apply(eval_model, model())({\"params\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc in: (64, 10000)\n",
      "Enc conv1: (64, 32)\n",
      "Enc conv2: (64, 64)\n",
      "Enc reshape: (64, 64)\n",
      "Dec in: (64, 20)\n",
      "Dec reshape1: (64, 10000, 64)\n",
      "Dec convt1: (64, 10000, 64)\n",
      "Dec convt2: (64, 10000, 32)\n",
      "Dec convt3, recon_x: (64, 10000, 1)\n",
      "recon_x flatten: (64, 10000)\n",
      "Enc in: (64, 10000)\n",
      "Enc conv1: (64, 32)\n",
      "Enc conv2: (64, 64)\n",
      "Enc reshape: (64, 64)\n",
      "Dec in: (64, 20)\n",
      "Dec reshape1: (64, 10000, 64)\n",
      "Dec convt1: (64, 10000, 64)\n",
      "Dec convt2: (64, 10000, 32)\n",
      "Dec convt3, recon_x: (64, 10000, 1)\n",
      "recon_x flatten: (64, 10000)\n",
      "Enc in: (2000, 10000)\n",
      "Enc conv1: (2000, 32)\n",
      "Enc conv2: (2000, 64)\n",
      "Enc reshape: (2000, 64)\n",
      "Dec in: (2000, 20)\n",
      "Dec reshape1: (2000, 10000, 64)\n",
      "Dec convt1: (2000, 10000, 64)\n",
      "Dec convt2: (2000, 10000, 32)\n",
      "Dec convt3, recon_x: (2000, 10000, 1)\n",
      "recon_x flatten: (2000, 10000)\n",
      "Dec in: (64, 20)\n",
      "Dec reshape1: (64, 10000, 64)\n",
      "Dec convt1: (64, 10000, 64)\n",
      "Dec convt2: (64, 10000, 32)\n",
      "Dec convt3, recon_x: (64, 10000, 1)\n",
      "recon_x flatten: (64, 10000)\n",
      "epoch: 1, ELBO: 355.3152, MSE: 349.9854, KLD: 5.3298\n"
     ]
    }
   ],
   "source": [
    "rng = random.key(seed)\n",
    "rng, state = init_model(rng)\n",
    "\n",
    "rng, z_key, eval_rng = random.split(rng, 3)\n",
    "\n",
    "z = random.normal(z_key, (batch_size, latent_dim))  # prior\n",
    "del z_key\n",
    "\n",
    "epoch_metrics = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in batches:\n",
    "        rng, key = random.split(rng)\n",
    "        state = train_step(state, batch, key)\n",
    "\n",
    "    metrics, comparison, samples = eval_f(state.params, v_test, z, eval_rng)\n",
    "    metrics[\"epoch\"] = epoch + 1\n",
    "    epoch_metrics.append(metrics)\n",
    "    print(f\"epoch: {epoch + 1}, ELBO: {metrics['elbo']:.4f}, MSE: {metrics['mse']:.4f}, KLD: {metrics['kld']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
